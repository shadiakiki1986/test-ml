{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM for lag\n",
    "\n",
    "- p5a: Copied from https://cmsdk.com/python/lstm--learn-a-simple-lag-in-the-data.html\n",
    "- p5b: my fixes\n",
    "- p5c: Use LSTM-based AE (i.e. no output, just unsupervised dimensionality reduction)\n",
    "- p5d: LSTM-based AE made more efficient?\n",
    "- p5e: LSTM-based AE on GPU + middle encoding layer\n",
    "- p5f: remove dropout and use optimizer=nadam\n",
    "- p5g: LSTM-60-20-1-20-60, LSTM-60-20-2-20-60\n",
    "\n",
    "Other references\n",
    "  - some LSTM AE code in Keras [issue 1401](https://github.com/fchollet/keras/issues/1401#issuecomment-309443122)\n",
    "    - http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "  - https://github.com/fchollet/keras/issues/5138#issuecomment-274893934"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### below model results summary\n",
    "\n",
    "with enc_dim=1 (using Dense layer):\n",
    "-    loss reaches 0.70 in  50 epochs and slows down dramatically, prediction does not display lags\n",
    "\n",
    "with enc_dim=None:\n",
    "-    loss reaches 0.01 in 150 epochs, prediction displays lags\n",
    "\n",
    "with enc_dim=10 (using Dense layer):\n",
    "- Epoch 65/200: 6s - loss: 0.1557 - val_loss: 0.1536\n",
    "  - predictions do not display lags\n",
    "  \n",
    "with enc_dim=10 (using LSTM layer):\n",
    "- Epoch 107/200 - loss: 0.0772 - val_loss: 0.0786\n",
    "  - 1st 2 predicted features display lag, 3rd feature still negligible\n",
    "- Epoch 107+200/200 - loss: 0.0714 - val_loss: 0.0711\n",
    "  - 1st 2 predicted features display lag, 3rd feature still negligible\n",
    "  \n",
    "add dropout\n",
    "- Epoch 67/200 - loss: 0.1838 - val_loss: 0.1620\n",
    "  - no visible correlations\n",
    "\n",
    "Tried and failed to make it more efficient\n",
    "- with enc_dim != None,\n",
    "  - try to pretrain without the intermediate stacked LSTM\n",
    "  - then train with it\n",
    "  - ref: https://stackoverflow.com/a/41661229/4126114\n",
    "- cannot instantiate network without encoding layer, train, instantiate with encoding, copy weights, train\n",
    "  - because the input to the output layer is different in these 2 cases\n",
    "  - will try to just freeze the encoding layer with `rnn_model.trainable = False` and train, then set to True, and train\n",
    "  - will also need to see if this is really more efficient  \n",
    "\n",
    "Run on GPU\n",
    "- super fast\n",
    "- enc_dim = 10, data length = 100e3\n",
    "  - Epoch 50+27/300   8s - loss: 0.2472 - val_loss: 0.2166\n",
    "    - reconstructed features 1 & 2 display lag, but 3 doesnt and is still small\n",
    "  - epoch 27+50+50+63    8s - loss: 0.1118 - val_loss: 0.0896\n",
    "    - features 1 & 2 lag is better, but 3 still not\n",
    "  - Epoch 27+50+50+63+50/300   8s - loss: 0.0995 - val_loss: 0.0825\n",
    "    - features 1 & 2 lag is almost perfect, but 3 still negligible\n",
    "\n",
    "Use nadam and remove dropout\n",
    "- [nadam](https://keras.io/optimizers/) says something about adam and momentum.\n",
    "    - note that the [keras docs](https://keras.io/getting-started/sequential-model-guide/) on stacked lstm just use rmsprop\n",
    "    - \"in my experience, adam is better than rmsprop\" ([ref](https://github.com/fchollet/keras/issues/1401#issuecomment-169295237))\n",
    "- Epoch 64/300    8s - loss: 0.0770 - val_loss: 0.0843\n",
    "\n",
    "LSTM-20-20\n",
    "- Epoch 40/300     6s - loss: 0.0239 - val_loss: 0.0207\n",
    "  - features 1 & 2 display lag very well, feature 3 is starting to display lag\n",
    "- Epoch 24/300     6s - loss: 0.0056 - val_loss: 0.0049\n",
    "\n",
    "Use `optimizer = Nadam(lr=1, schedule_decay=0.0)`\n",
    "- is this like increasing gradient step size?\n",
    "\n",
    "LSTM-20-1-20\n",
    "- reached 0.73 after 40 epochs\n",
    "- seems to have plateaud\n",
    "\n",
    "LSTM-1\n",
    "- Epoch 41/300    2s - loss: 0.6639 - val_loss: 0.6722\n",
    "- seems to have plateaud\n",
    "\n",
    "LSTM-60 + LSTM-1 + LSTM-60\n",
    "- after 300 epochs, loss = 0.69\n",
    "\n",
    "LSTM-20-20, optimizer = adam\n",
    "- Epoch 44/300     6s - loss: 0.0144 - val_loss: 0.0134\n",
    "- Epoch 75/300     6s - loss: 0.0032 - val_loss: 0.0037\n",
    "- reconstructed features are very very close to originals (with display of lag)\n",
    "\n",
    "LSTM-60-20-60, optimizer = adam\n",
    "- Epoch 39/300    20s - loss: 0.0051 - val_loss: 0.0041\n",
    "- R2 = 0.997 (reconstructed features show lag)\n",
    "\n",
    "LSTM-60-20-1-20-60, optimizer = adam\n",
    "- Epoch 14/300    24s - loss: 0.3688 - val_loss: 0.3734\n",
    "  - R2 = 0.45\n",
    "  - some visible lag, but still early to see\n",
    "- Epoch 14+10/300    24s - loss: 0.2828 - val_loss: 0.2925\n",
    "  - single encoded feature still not meaningful\n",
    "- Epoch 14+10+20/300    23s - loss: 0.1952 - val_loss: 0.1926\n",
    "  - R2 = 0.58\n",
    "  - more visible lag, but still 3rd feature is negligible\n",
    "  - single encoded feature starting to look like the single important component\n",
    "- Epoch 14+10+20+41/300    24s - loss: 0.1209 - val_loss: 0.1116\n",
    "  - R2 = 0.63\n",
    "  - first 2 features correlation is visible, last feature yet to see\n",
    "  - encoded feature yet to become more like single component\n",
    "- Epoch 14+10+20+41+6/300    24s - loss: 0.1066 - val_loss: 0.1064\n",
    "- Epoch 14+10+20+41+6+116/300  24s - loss: 0.0670 - val_loss: 0.0691\n",
    "  - R2 = 0.6\n",
    "  - first 2 feature correlation visible, 3rd feature negligible\n",
    "  - encoded feature not trustworthy because in/out not perfectly replicated\n",
    "\n",
    "LSTM-60-20-2-20-60\n",
    "- Epoch 300/300 72s - loss: 0.0011 - val_loss: 7.8084e-04\n",
    "- R2 = 0.9992\n",
    "- got 2 encoded features that don't look lagged\n",
    "\n",
    "LSTM-60-20-3-20-60\n",
    "- Epoch 300/300   55s - loss: 6.3800e-04 - val_loss: 3.0157e-04\n",
    "- R2 = 0.9998\n",
    "- got 3 encoded features that don't look lagged\n",
    "\n",
    "LSTM-60-20-4-20-60\n",
    "- Epoch 300/300  39s - loss: 7.3666e-04 - val_loss: 9.2248e-04\n",
    "- R2 = 0.9995\n",
    "- got 4 encoded features that don't look lagged\n",
    "\n",
    "LSTM-60-20-10-5-1-5-10-20-60\n",
    "- Reconstruction got 2 features well, but 3rd feature stayed flat\n",
    "- Epoch 300/300   30s - loss: 0.0677 - val_loss: 0.0688\n",
    "- R2 = 0.666441571972\n",
    "\n",
    "LSTM-20-2-20\n",
    "- 300 epochs, loss = 0.60\n",
    "- R2 = 0.2\n",
    "\n",
    "LSTM-60-2-60\n",
    "- 300 epochs, loss = 0.56\n",
    "- R2 = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM, GRU\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "import keras\n",
    "def strided_app(a, L, S ):  #To get X in the right format expected by Keras\n",
    "    nrows = ((a.size-L)//S)+1\n",
    "    n = a.strides[0]\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=(nrows,L), strides=(S*n,n))\n",
    "\n",
    "import utils\n",
    "import utils2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_samples = int(100e3) # FIXME # 100e3\n",
    "X = np.random.randn(nb_samples)\n",
    "# generate data with 3-lag correlation: X(t-1), X(t), and X(t+1)\n",
    "timesteps = 3\n",
    "\n",
    "X_model = strided_app(X, timesteps, 1)\n",
    "print('model', X_model.shape)\n",
    "utils.myPlot(X_model[:20],2)\n",
    "\n",
    "# The first striding above was done in order to generate the input data with a built-in lag relationship\n",
    "# Now need to stride again because we need to pretend that we do not know that the input has this correlation\n",
    "# Without this re-striding, it's like telling the model to get y=X\n",
    "look_back = 10\n",
    "X_calib      = utils2._load_data_strides(X_model,       n_prev=look_back)\n",
    "print('calib', X_calib.shape)\n",
    "utils.myPlot(X_calib[:20,0,:],2) # same as earlier plot\n",
    "utils.myPlot(X_calib[:20,1,:],2) # lagged version of above plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT train/validation/test\n",
    "nb_samples_calib = X_calib.shape[0]\n",
    "nb_train = int(nb_samples_calib * 0.7)\n",
    "nb_val = int(nb_samples_calib * 0.85)\n",
    "# strided version\n",
    "Xc_train = X_calib[: nb_train]\n",
    "Xc_validation = X_calib[nb_train : nb_val]\n",
    "Xc_test = X_calib[nb_val:]\n",
    "# unstrided version\n",
    "Xm_train = X_model[: nb_train]\n",
    "Xm_validation = X_model[nb_train : nb_val]\n",
    "Xm_test = X_model[nb_val:]\n",
    "\n",
    "print('strided',   Xc_train.shape, Xc_validation.shape, Xc_test.shape)\n",
    "print('unstrided', Xm_train.shape, Xm_validation.shape, Xm_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note 1: no dropout\n",
    "# Note 2: add encoded dimension\n",
    "# Using build_lstm_vanilla with enc_dim doesn't work as the output features cannot be lagged from each other\n",
    "#    with enc_dim = 1, and no LSTM after enc_dim: until epoch=100, loss: 0.6737 - val_loss: 0.6645\n",
    "#                                                 until epoch=200, loss: 0.6724 - val_loss: 0.6615\n",
    "#enc_dim = 1\n",
    "#model = utils2.build_lstm_vanilla(in_neurons=Xc_train.shape[2], out_neurons=Xm_train.shape[1], lstm_dim=20, enc_dim=enc_dim)\n",
    "#----------------------------\n",
    "# from keras.optimizers import Nadam\n",
    "# lstm_dim = 60 # FIXME 60 20\n",
    "# enc_dim = [20,2,20] # FIXME 10 1 None 20\n",
    "# optimizer = 'adam' # FIXME Nadam(lr=1, schedule_decay=0.0) # lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n",
    "# model = utils2.build_lstm_ae(in_neurons=Xc_train.shape[2], lstm_dim=lstm_dim, look_back=look_back, enc_dim=enc_dim, optimizer=optimizer)\n",
    "#----------------------------\n",
    "enc_dim1 = 60\n",
    "enc_dim2 = 20\n",
    "enc_dim3 = 2\n",
    "optimizer = 'adam' # FIXME Nadam(lr=1, schedule_decay=0.0) # lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n",
    "model = utils.buildNetwork2_deep(input_shape=Xc_train.shape[2], enc_dim1=enc_dim1, enc_dim2=enc_dim2, enc_dim3=enc_dim3, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 300 # FIXME # 100\n",
    "\n",
    "#weights_path = 'weights.h5'\n",
    "#early_stopping = EarlyStopping(monitor='val_loss',\n",
    "#                           patience=100)\n",
    "#checkpointer = ModelCheckpoint(filepath=weights_path,\n",
    "#                           verbose=2,\n",
    "#                           save_best_only=True)\n",
    "#callbacks = [early_stopping, checkpointer]\n",
    "history = model.fit(Xc_train,\n",
    "                    Xc_train, # the TimeDistributed in the model allows me to train again Xc instead of Xm\n",
    "                    epochs = epochs, # FIXME # 10000,\n",
    "                    callbacks = [], #callbacks,\n",
    "                    verbose = 2,\n",
    "                    # since stateless, make sure this is larger than the look-back=10 and the generated lag=3\n",
    "                    batch_size = 500, # FIXME # 100\n",
    "                    validation_data = (Xc_validation,\n",
    "                                       Xc_validation),\n",
    "                    shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def myPlot(X):\n",
    "    plt.plot(X)\n",
    "    plt.show()\n",
    "\n",
    "#def R2(X, Y, model):\n",
    "#    Y_mean = np.mean(Y, axis=0)\n",
    "#    pred = model.predict(X)\n",
    "#    res = np.sum(np.square(Y - pred))\n",
    "#    tot = np.sum(np.square(Y - Y_mean))\n",
    "#    r2 = 1 - res / tot\n",
    "#    return r2\n",
    "\n",
    "def R2(X, Y, model):\n",
    "    Y_mean = np.mean(Y, axis=0)\n",
    "    pred = model.predict(X)\n",
    "    print('X', X.shape, 'Y', Y.shape, 'pred', pred.shape)\n",
    "    #pred = pred.reshape(Y.shape[0])\n",
    "    pred = pred[:,0,:]\n",
    "    print('reshaped pred', pred.shape)\n",
    "    res = np.sum(np.square(Y - pred))\n",
    "    tot = np.sum(np.square(Y - Y_mean))\n",
    "    r2 = 1 - res / tot\n",
    "    myPlot(X[:20,0])\n",
    "    myPlot(Y[:20])\n",
    "    myPlot(pred[:20])\n",
    "    return r2\n",
    "\n",
    "#model.load_weights(weights_path)\n",
    "print(R2(Xc_train,      Xm_train,      model))\n",
    "print(R2(Xc_validation, Xm_validation, model))\n",
    "# some dimensionality problems\n",
    "# print(R2(Xc_test,       Xm_test,       model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check encoded signal: expect detection of 1 of the lagged features in X\n",
    "from keras.models import Model\n",
    "lstm_enc = Model(inputs=model.input, outputs=model.layers[2].output)\n",
    "lstm_enc.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")\n",
    "lstm_enc.summary()\n",
    "\n",
    "X_enc = lstm_enc.predict(Xc_test)\n",
    "print(Xc_test.shape, Xm_test.shape, X_enc.shape)\n",
    "utils.myPlot(Xc_test[:20,0,:],0)\n",
    "utils.myPlot(Xm_test[:20],0)\n",
    "utils.myPlot(X_enc[:20,0],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
